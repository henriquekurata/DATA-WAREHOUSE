# üöÄ ***Pipeline de dados com Apache Airflow***

## **Descri√ß√£o do Projeto:**
Este projeto tem como objetivo criar e automatizar um pipeline de dados usando o **Apache Airflow** e **Python** para criar e inserir dados no **PostgreSQL**. A solu√ß√£o inclui a configura√ß√£o de cont√™ineres Docker para isolar e gerenciar o ambiente do PostgreSQL e do Apache Airflow, al√©m da implementa√ß√£o de uma DAG (Directed Acyclic Graph) para gerenciar o processo ETL (Extract, Transform, Load).

## üõ†Ô∏è **Tecnologias Utilizadas**:
* Docker: Plataforma para criar e gerenciar containers, isolando os ambientes de execu√ß√£o do PostgreSQL e Apache Airflow;
* PostgreSQL: Banco de dados relacional onde os dados s√£o armazenados e manipulados;
* pgAdmin: Interface gr√°fica para gerenciar e administrar o PostgreSQL;
* Apache Airflow: Ferramenta de orquestra√ß√£o para automatizar e gerenciar pipelines de dados;
* Anaconda: Distribui√ß√£o de Python que facilita a gest√£o de pacotes e ambientes de desenvolvimento.

## **Principais Funcionalidades**
### 1. **Cria√ß√£o e Configura√ß√£o de Cont√™ineres Docker**
   - **PostgreSQL**: Configura√ß√£o de um cont√™iner Docker para o banco de dados relacional PostgreSQL, incluindo a cria√ß√£o do banco de dados e schema necess√°rios.
   - **Apache Airflow**: Configura√ß√£o de um cont√™iner Docker para o Apache Airflow, incluindo a cria√ß√£o de uma rede dedicada para comunica√ß√£o entre cont√™ineres e a configura√ß√£o inicial do Airflow.

### 2. **Gerenciamento de Cont√™ineres com Docker**
   - **Cria√ß√£o de Imagens e Cont√™ineres**: Processos automatizados para criar e iniciar imagens e cont√™ineres necess√°rios para PostgreSQL e Airflow.
   - **Comunica√ß√£o entre Cont√™ineres**: Configura√ß√£o de redes e conex√µes para permitir que o Airflow se comunique com o banco de dados PostgreSQL.

### 3. **Cria√ß√£o e Configura√ß√£o de DAG no Apache Airflow**
   - **Defini√ß√£o de DAG (Directed Acyclic Graph)**: Cria√ß√£o de uma DAG no Apache Airflow para gerenciar o pipeline de dados, incluindo tarefas de cria√ß√£o e inser√ß√£o de dados.
   - **Tarefas ETL (Extract, Transform, Load)**: Implementa√ß√£o de tarefas espec√≠ficas para criar tabelas e inserir dados no PostgreSQL usando o operador `PostgresOperator` do Airflow.

### 4. **Automatiza√ß√£o do Pipeline de Dados**
   - **Execu√ß√£o Programada**: Configura√ß√£o da DAG para ser executada em um hor√°rio espec√≠fico ou manualmente atrav√©s da interface do Airflow.

### 5. **Configura√ß√£o e Monitoramento**
   - **Interface Web do Airflow**: Acesso ao painel do Apache Airflow para monitorar a execu√ß√£o das DAGs, visualizar logs e gerenciar tarefas.
   - **Cria√ß√£o de Conex√µes no Airflow**: Configura√ß√£o de conex√µes no Airflow para se comunicar com o PostgreSQL, incluindo a defini√ß√£o de credenciais e detalhes de conex√£o.




## üìã **Descri√ß√£o do Processo**
* Criar imagem e container para o banco de dados do DW;
* Criar imagem e containers para o Apache Airflow;
* Configurar a comunica√ß√£o entre as redes de containers (PostgreSQL e Airflow);
* Criar a connetion ID no Airlow;
* Criar a DAG;
* Inserir a DAG dentro da pasta raiz na m√°quina local do Airflow;
* Disparar a DAG.


## ‚öôÔ∏è **Comandos**:

### Preparando o Container Docker Para o Banco de Dados do DW

Execute os comandos abaixo no terminal ou prompt de comando para baixar a imagem e inicializar o Postgres:

docker pull postgres

docker run --name dbdsa -p 5433:5432 -e POSTGRES_USER=dsalabdw -e POSTGRES_PASSWORD=dsalabdw123 -e POSTGRES_DB=dwdb -d postgres



### Configurar o SGBD

Acesse o Postgres pelo **pgAdmin** e crie:

- Name SGBD Pgadmin: **Lab5**
- Schema: **dsalabdw**

---

### Preparando os Containers Docker para o Apache Airflow

1. Crie uma pasta vazia na raiz com o nome `Airflow` na m√°quina local.
2. Navegue at√© a pasta `Airflow` usando o terminal ou CMD.
3. Siga a documenta√ß√£o oficial do Airflow no link:  
   [Documenta√ß√£o do Airflow com Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
   
Execute os seguintes comandos dentro da pasta `Airflow`:



curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.3/docker-compose.yaml'

mkdir -p ./dags ./logs ./plugins ./config

echo -e "AIRFLOW_UID=$(id -u)" > .env

docker compose up airflow-init

docker compose up


Agora, com a rede de containers em execu√ß√£o, acesse o painel do Airflow via `localhost:8080` usando as credenciais padr√£o:

- **User**: airflow  
- **Password**: airflow

---

### Configurando a Comunica√ß√£o entre Containers (Docker PostgreSQL e Airflow)

#### Passo 1: Listar as Redes Docker

docker network ls


#### Passo 2: Inspecionar o Container do Banco de Dados


docker inspect dbdsa


#### Passo 3: Extrair Detalhes sobre a Rede do Container


docker inspect dbdsa -f "{{json .NetworkSettings.Networks }}"


#### Passo 4: Inspecionar a Rede de Todos os Containers Simultaneamente


docker ps --format '{{ .ID }} {{ .Names }} {{ json .Networks }}'


#### Passo 5: Inspecionar a Rede do Airflow e da Rede `bridge`


docker network inspect airflow_default

docker network inspect bridge


#### Passo 6: Instalar Ferramentas de Rede no Container para Testar Conex√£o

Dentro do container do PostgreSQL, execute os comandos:

apt-get update

apt-get install net-tools

apt-get install iputils-ping


#### Passo 7: Fazer Teste de Conex√£o entre o Container `dbdsa` e o Webserver do Apache Airflow


ifconfig

ping


#### Passo 8: Colocar o Banco de Dados na Mesma Rede do Airflow

1. Desconectar o container `dbdsa` da rede `bridge`:

docker network disconnect bridge dbdsa


2. Conectar o container `dbdsa` na rede do Airflow:

docker network connect airflow_default dbdsa

---

### Criar a Conex√£o no Airflow

Ao acessar o Apache Airflow √© necess√°rio criar a conex√£o (menu > connetcion): 

- **Name connetion id**: Lab5DW  
- Preencher os campos com os seguintes valores:
  - **Host**: Utilize o comando `ifconfig` para adicionar o INET (IP da m√°quina do SGBD)
  - **Schema**: Nome do banco de dados
  - **Port**: Porta do container Docker

---



## Job ETL (Arquivo: `job_etl_lab5`)
```python

# Imports
import airflow
from datetime import timedelta
from airflow import DAG
from airflow.operators.postgres_operator import PostgresOperator
from airflow.utils.dates import days_ago

# Argumentos
args = {'owner': 'airflow'}

# Argumentos default
default_args = {
    'owner': 'airflow',    
    #'start_date': airflow.utils.dates.days_ago(2),
    #'end_date': datetime(),
    #'depends_on_past': False,
    #'email': ['airflow@example.com'],
    #'email_on_failure': False,
    #'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes = 5),
}

# Cria a DAG
dag_lab5_dsa = DAG(dag_id = "Lab5",
                   default_args = args,
                   # schedule_interval='0 0 * * *',
                   schedule_interval = '@once',  
                   dagrun_timeout = timedelta(minutes = 60),
                   description = 'Job ETL de Carga no DW com Airflow',
                   start_date = airflow.utils.dates.days_ago(1)
)

# Instru√ß√£o SQL de cria√ß√£o de tabela
sql_cria_tabela = """CREATE TABLE IF NOT EXISTS tb_funcionarios (id INT NOT NULL, nome VARCHAR(250) NOT NULL, departamento VARCHAR(250) NOT NULL);"""

# Tarefa de cria√ß√£o da tabela
cria_tabela = PostgresOperator(sql = sql_cria_tabela,
                               task_id = "tarefa_cria_tabela",
                               postgres_conn_id = "Lab5DW",
                               dag = dag_lab5_dsa
)

# Instru√ß√£o SQL de insert na tabela
sql_insere_dados = """
insert into tb_funcionarios (id, nome, departamento) values (1000, 'Bob', 'Marketing'), (1001, 'Maria', 'Contabilidade'),(1002, 'Jeremias', 'Engenharia de Dados'), (1003, 'Messi', 'Marketing') ;"""

# Tarefa de insert na tabela
insere_dados = PostgresOperator(sql = sql_insere_dados,
                                task_id = "tarefa_insere_dados",
                                postgres_conn_id = "Lab5DW",
                                dag = dag_lab5_dsa
)

# Fluxo da DAG
cria_tabela >> insere_dados

# Bloco main
if __name__ == "__main__":
    dag_lab5_dsa.cli()

```

### Observa√ß√µes Finais:

* Coloque o arquivo job_etl_lab5 dentro da pasta AIRFLOW/dag criada na raiz da m√°quina local.
  
* Assim que o arquivo estiver na pasta correta, a DAG automaticamente ir√° aparecer na interface do Airflow (porta 8080).
  
* Dispare a trigger da DAG no Airflow para que os dados sejam criados e inseridos no PostgreSQL.


---
## Contato

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)