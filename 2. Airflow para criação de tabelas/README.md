# ğŸš€ ***Pipeline de dados com Apache Airflow***

## **DescriÃ§Ã£o do Projeto:**
Este projeto tem como objetivo criar e automatizar um pipeline de dados usando o **Apache Airflow** e **Python** para criar e inserir dados no **PostgreSQL**. A soluÃ§Ã£o inclui a configuraÃ§Ã£o de contÃªineres Docker para isolar e gerenciar o ambiente do PostgreSQL e do Apache Airflow, alÃ©m da implementaÃ§Ã£o de uma DAG (Directed Acyclic Graph) para gerenciar o processo ETL (Extract, Transform, Load).

## ğŸ› ï¸ **Tecnologias Utilizadas**:
* Docker: Plataforma para criar e gerenciar containers, isolando os ambientes de execuÃ§Ã£o do PostgreSQL e Apache Airflow;
* PostgreSQL: Banco de dados relacional onde os dados sÃ£o armazenados e manipulados;
* pgAdmin: Interface grÃ¡fica para gerenciar e administrar o PostgreSQL;
* Apache Airflow: Ferramenta de orquestraÃ§Ã£o para automatizar e gerenciar pipelines de dados;
* Anaconda: DistribuiÃ§Ã£o de Python que facilita a gestÃ£o de pacotes e ambientes de desenvolvimento.

## **Principais Funcionalidades**
### 1. **CriaÃ§Ã£o e ConfiguraÃ§Ã£o de ContÃªineres Docker**
   - **PostgreSQL**: ConfiguraÃ§Ã£o de um contÃªiner Docker para o banco de dados relacional PostgreSQL, incluindo a criaÃ§Ã£o do banco de dados e schema necessÃ¡rios.
   - **Apache Airflow**: ConfiguraÃ§Ã£o de um contÃªiner Docker para o Apache Airflow, incluindo a criaÃ§Ã£o de uma rede dedicada para comunicaÃ§Ã£o entre contÃªineres e a configuraÃ§Ã£o inicial do Airflow.

### 2. **Gerenciamento de ContÃªineres com Docker**
   - **CriaÃ§Ã£o de Imagens e ContÃªineres**: Processos automatizados para criar e iniciar imagens e contÃªineres necessÃ¡rios para PostgreSQL e Airflow.
   - **ComunicaÃ§Ã£o entre ContÃªineres**: ConfiguraÃ§Ã£o de redes e conexÃµes para permitir que o Airflow se comunique com o banco de dados PostgreSQL.

### 3. **CriaÃ§Ã£o e ConfiguraÃ§Ã£o de DAG no Apache Airflow**
   - **DefiniÃ§Ã£o de DAG (Directed Acyclic Graph)**: CriaÃ§Ã£o de uma DAG no Apache Airflow para gerenciar o pipeline de dados, incluindo tarefas de criaÃ§Ã£o e inserÃ§Ã£o de dados.
   - **Tarefas ETL (Extract, Transform, Load)**: ImplementaÃ§Ã£o de tarefas especÃ­ficas para criar tabelas e inserir dados no PostgreSQL usando o operador `PostgresOperator` do Airflow.

### 4. **AutomatizaÃ§Ã£o do Pipeline de Dados**
   - **ExecuÃ§Ã£o Programada**: ConfiguraÃ§Ã£o da DAG para ser executada em um horÃ¡rio especÃ­fico ou manualmente atravÃ©s da interface do Airflow.

### 5. **ConfiguraÃ§Ã£o e Monitoramento**
   - **Interface Web do Airflow**: Acesso ao painel do Apache Airflow para monitorar a execuÃ§Ã£o das DAGs, visualizar logs e gerenciar tarefas.
   - **CriaÃ§Ã£o de ConexÃµes no Airflow**: ConfiguraÃ§Ã£o de conexÃµes no Airflow para se comunicar com o PostgreSQL, incluindo a definiÃ§Ã£o de credenciais e detalhes de conexÃ£o.




## ğŸ“‹ **DescriÃ§Ã£o do Processo**
* Criar imagem e container para o banco de dados do DW;
* Criar imagem e containers para o Apache Airflow;
* Configurar a comunicaÃ§Ã£o entre as redes de containers (PostgreSQL e Airflow);
* Criar a connetion ID no Airlow;
* Criar a DAG;
* Inserir a DAG dentro da pasta raiz na mÃ¡quina local do Airflow;
* Disparar a DAG.


## âš™ï¸ **Comandos**:

### Preparando o Container Docker Para o Banco de Dados do DW

Execute os comandos abaixo no terminal ou prompt de comando para baixar a imagem e inicializar o Postgres:

docker pull postgres

docker run --name dbdsa -p 5433:5432 -e POSTGRES_USER=dsalabdw -e POSTGRES_PASSWORD=dsalabdw123 -e POSTGRES_DB=dwdb -d postgres



### Configurar o SGBD

Acesse o Postgres pelo **pgAdmin** e crie:

- Name SGBD Pgadmin: **Lab5**
- Schema: **dsalabdw**

---

### Preparando os Containers Docker para o Apache Airflow

1. Crie uma pasta vazia na raiz com o nome `Airflow` na mÃ¡quina local.
2. Navegue atÃ© a pasta `Airflow` usando o terminal ou CMD.
3. Siga a documentaÃ§Ã£o oficial do Airflow no link:  
   [DocumentaÃ§Ã£o do Airflow com Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
   
Execute os seguintes comandos dentro da pasta `Airflow`:



curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.3/docker-compose.yaml'

mkdir -p ./dags ./logs ./plugins ./config

echo -e "AIRFLOW_UID=$(id -u)" > .env

docker compose up airflow-init

docker compose up


Agora, com a rede de containers em execuÃ§Ã£o, acesse o painel do Airflow via `localhost:8080` usando as credenciais padrÃ£o:

- **User**: airflow  
- **Password**: airflow

---

### Configurando a ComunicaÃ§Ã£o entre Containers (Docker PostgreSQL e Airflow)

#### Passo 1: Listar as Redes Docker

docker network ls


#### Passo 2: Inspecionar o Container do Banco de Dados


docker inspect dbdsa


#### Passo 3: Extrair Detalhes sobre a Rede do Container


docker inspect dbdsa -f "{{json .NetworkSettings.Networks }}"


#### Passo 4: Inspecionar a Rede de Todos os Containers Simultaneamente


docker ps --format '{{ .ID }} {{ .Names }} {{ json .Networks }}'


#### Passo 5: Inspecionar a Rede do Airflow e da Rede `bridge`


docker network inspect airflow_default

docker network inspect bridge


#### Passo 6: Instalar Ferramentas de Rede no Container para Testar ConexÃ£o

Dentro do container do PostgreSQL, execute os comandos:

apt-get update

apt-get install net-tools

apt-get install iputils-ping


#### Passo 7: Fazer Teste de ConexÃ£o entre o Container `dbdsa` e o Webserver do Apache Airflow


ifconfig

ping


#### Passo 8: Colocar o Banco de Dados na Mesma Rede do Airflow

1. Desconectar o container `dbdsa` da rede `bridge`:

docker network disconnect bridge dbdsa


2. Conectar o container `dbdsa` na rede do Airflow:

docker network connect airflow_default dbdsa

---

### Criar a ConexÃ£o no Airflow

Ao acessar o Apache Airflow Ã© necessÃ¡rio criar a conexÃ£o (menu > connetcion): 

- **Name connetion id**: Lab5DW  
- Preencher os campos com os seguintes valores:
  - **Host**: Utilize o comando `ifconfig` para adicionar o INET (IP da mÃ¡quina do SGBD)
  - **Schema**: Nome do banco de dados
  - **Port**: Porta do container Docker

---



## Job ETL (Arquivo: `job_etl_lab5`)
```python

# Imports
import airflow
from datetime import timedelta
from airflow import DAG
from airflow.operators.postgres_operator import PostgresOperator
from airflow.utils.dates import days_ago

# Argumentos
args = {'owner': 'airflow'}

# Argumentos default
default_args = {
    'owner': 'airflow',    
    #'start_date': airflow.utils.dates.days_ago(2),
    #'end_date': datetime(),
    #'depends_on_past': False,
    #'email': ['airflow@example.com'],
    #'email_on_failure': False,
    #'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes = 5),
}

# Cria a DAG
dag_lab5_dsa = DAG(dag_id = "Lab5",
                   default_args = args,
                   # schedule_interval='0 0 * * *',
                   schedule_interval = '@once',  
                   dagrun_timeout = timedelta(minutes = 60),
                   description = 'Job ETL de Carga no DW com Airflow',
                   start_date = airflow.utils.dates.days_ago(1)
)

# InstruÃ§Ã£o SQL de criaÃ§Ã£o de tabela
sql_cria_tabela = """CREATE TABLE IF NOT EXISTS tb_funcionarios (id INT NOT NULL, nome VARCHAR(250) NOT NULL, departamento VARCHAR(250) NOT NULL);"""

# Tarefa de criaÃ§Ã£o da tabela
cria_tabela = PostgresOperator(sql = sql_cria_tabela,
                               task_id = "tarefa_cria_tabela",
                               postgres_conn_id = "Lab5DW",
                               dag = dag_lab5_dsa
)

# InstruÃ§Ã£o SQL de insert na tabela
sql_insere_dados = """
insert into tb_funcionarios (id, nome, departamento) values (1000, 'Bob', 'Marketing'), (1001, 'Maria', 'Contabilidade'),(1002, 'Jeremias', 'Engenharia de Dados'), (1003, 'Messi', 'Marketing') ;"""

# Tarefa de insert na tabela
insere_dados = PostgresOperator(sql = sql_insere_dados,
                                task_id = "tarefa_insere_dados",
                                postgres_conn_id = "Lab5DW",
                                dag = dag_lab5_dsa
)

# Fluxo da DAG
cria_tabela >> insere_dados

# Bloco main
if __name__ == "__main__":
    dag_lab5_dsa.cli()

```

### ObservaÃ§Ãµes Finais:

* Coloque o arquivo job_etl_lab5 dentro da pasta AIRFLOW/dag criada na raiz da mÃ¡quina local.
  
* Assim que o arquivo estiver na pasta correta, a DAG automaticamente irÃ¡ aparecer na interface do Airflow (porta 8080).
  
* Dispare a trigger da DAG no Airflow para que os dados sejam criados e inseridos no PostgreSQL.


---
## Contato

Se tiver dÃºvidas ou sugestÃµes sobre o projeto, entre em contato comigo:

- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- ğŸ± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)